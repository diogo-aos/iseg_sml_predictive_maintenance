{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236de8bb-42bd-46e5-8f69-f8a23dbea378",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "- [X] send email to Errol - does the team that installed an asset do all other maintenance related events for the same asset? Useful for determining team productivity and effectiveness.\n",
    "- [X] Meaning of previous_repairs and previous_unplanned in assets table.\n",
    "At first it seemed that it was simply the values of the last row from the events table, but it doesn't seem to match up.\n",
    "- [ ] Meaning of non-zero previous_repairs and previous_unplanned in first event after installation.\n",
    "- [X] Transform datetimes and time periods to numeric data\n",
    "- [X] Augment assets data with events statistics: number of replacements, number of repairs, average time between events, ...?\n",
    "- [ ] Save the samples that are not included for the model training and testing (from the down sampling) and use them for testing. Will make a very large test set.\n",
    "\n",
    "\n",
    "`01MAI`\n",
    "- Trained the dataset with a Random Forest Classifier on the horizon 30 class. Very high accuracy.\n",
    "- Looked at the class distribution and observed that it was very imbalanced.\n",
    "- Downsamples to have a 50/50 split.\n",
    "- Retrained. Very high accuracy.\n",
    "\n",
    "\n",
    "Source:\n",
    "- Predicteve maintenance\n",
    "  - [predictive maintenance article - towardsdatascience](https://towardsdatascience.com/how-to-implement-machine-learning-for-predictive-maintenance-4633cdbe4860)\n",
    "- Imbalanced data\n",
    "  - [dealing with imbalanced class distribution](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n",
    "  - [what is imbalanced data](https://machinelearningmastery.com/what-is-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0086f-4fd4-4044-b7bf-72ebd1b0e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# staDecisionTreeClassifierd library\n",
    "from typing import List, Union, Optional, Tuple\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "# data and viz\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# numeric\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# data prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# metrics\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import tree\n",
    "\n",
    "# prod\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a64c3-38b1-4e47-ac79-99da772c2a0e",
   "metadata": {},
   "source": [
    "# 1. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee7bb2-65ac-470f-b2b5-3e9b2e2389f1",
   "metadata": {},
   "source": [
    "## 1.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bbde42-74bb-40de-b6c9-321a63a37b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles = !ls data/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328272fc-1153-4da5-8540-24151485d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for fn in datafiles:\n",
    "    dataset_name = fn.split('/')[-1].rstrip('.csv')\n",
    "    datasets[dataset_name] = pd.read_csv(fn)\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    print('\\n'*3, name, '\\n')\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72b434-d06f-4063-a922-ae7a48ce27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "(datasets['replacement_data'].shape[0] + datasets['repair_data'].shape[0]) == datasets['planned_data'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ae5e3-7e0c-4343-81b5-c623bc3ae82c",
   "metadata": {},
   "source": [
    "The planned data seems to contain all the repair and replacement events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39271781-3ffd-4e74-a57a-07810e55708d",
   "metadata": {},
   "source": [
    "## 1.2. Join all datasets in 2 tables (events, assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4651d-6eb6-4c97-b2f1-7c7dbb601c0a",
   "metadata": {},
   "source": [
    "We seem to have 2 types of data. We'll start the process of combining all tables into 2 tables for each of these types:\n",
    "- time series data for maintenance events (\"events\");\n",
    "- and assets' attributes data (\"assets\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e485b77-8779-4b03-92d8-840c02c436c7",
   "metadata": {},
   "source": [
    "### 1.2.1. Join events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33587591-2343-4cb5-b262-9355b434ff4a",
   "metadata": {},
   "source": [
    "To combine these tables, we will:\n",
    "1. We'll need to first add a column to repair_data and replacement_data to indicate the type of event.\n",
    "2. All columns are the same, so we can concatenate repair and replacement events.\n",
    "3. Inner join events with planned data. We just need the planned column.\n",
    "\n",
    "Let's call this new dataset \"events\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790ddb0-ef7c-4883-952b-0ddf66ea6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['replacement_data']['type'] = 'replacement'\n",
    "datasets['repair_data']['type'] = 'repair'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09a18e-8bba-4dba-8189-a416964f6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['all_events_data'] = pd.concat([datasets['replacement_data'], datasets['repair_data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a830ce7-0d3c-4105-b2bf-3bbaaa8ec5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.merge(datasets['all_events_data'], datasets['planned_data'], how='inner', on=['event_id', 'asset_id', 'event_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce5ef1-38e2-49e0-a8f3-ebf0d5d998ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['event_date', 'installed_date']\n",
    "for col in date_cols:\n",
    "    events[col] = pd.to_datetime(events[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082aa4a-bf9a-416d-941d-6e0ac60e8d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.sort_values(by=['asset_id', 'event_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd26a75-740c-4801-b2ee-e8999c448754",
   "metadata": {},
   "source": [
    "### 1.2.2 Join assets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34037067-e86b-4a46-b22c-7ee71e2367f9",
   "metadata": {},
   "source": [
    "Let's join the attributes of the assets in a single table:\n",
    "- asset_attribute_data_general\n",
    "- asset_attribute_data_usage\n",
    "- asset_attribute_data_weather\n",
    "- asset_data\n",
    "\n",
    "Let's call this new dataset \"assets\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c7585-4f59-42de-a02d-710b2a027f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = pd.merge(datasets['asset_attribute_data_general'], datasets['asset_attribute_data_usage'], on='asset_id')\n",
    "assets = pd.merge(assets, datasets['asset_attribute_data_weather'], on='asset_id')\n",
    "assets = pd.merge(assets, datasets['asset_data'], on='asset_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fcfaf8-7455-4e57-81cc-0395fd4ef3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['end_date', 'start_date']\n",
    "for col in date_cols:\n",
    "    assets[col] = pd.to_datetime(assets[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5bc660-6710-4e64-a170-5c3a1205e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets['total_useful_life'] = assets['end_date'] - assets['start_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cfbf4d-98c7-4ae2-b491-0b1f5082e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = assets.rename(columns={'previous_repairs': 'start_previous_repairs',\n",
    "               'previous_unplanned': 'start_previous_unplanned'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93044aed-44b2-45a0-b2b6-8dcdd18796b2",
   "metadata": {},
   "source": [
    "## 1.3. Check for gaps in datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e8a1b-f6a9-4a0b-af6b-d7e3fd76f0eb",
   "metadata": {},
   "source": [
    "Now we have only 2 tables to work with. One refers to data about maintenance events, the other about asset attributes.\n",
    "\n",
    "Let's check if there are gaps in the data:\n",
    "- Do all assets have maintenance events? If not, why?\n",
    "- Are there events refering to missing assets? These might need to be discarded depending on the following analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc23118-47fd-4a72-994f-0f700975e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do all assets have maintenance events?\n",
    "assets_that_broke = events['asset_id'].unique()\n",
    "print(f'{len(assets_that_broke)}\\t assets that have replacement or repair events')\n",
    "\n",
    "print(f'{assets.shape[0]}\\t assets')\n",
    "\n",
    "assets_without_events = assets[assets['asset_id'].isin(assets_that_broke) == False]\n",
    "print(f'{assets_without_events.shape[0]}\\t assets without events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe0ccc3-6ebf-43dd-88b3-9681a3e4865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_without_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffae2a-e33e-46eb-bf7c-4a8102abf344",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3bec1b-4c88-44b8-a024-6539a06df30c",
   "metadata": {},
   "source": [
    "Of the 200 registered assets, we have maintenance events on 194.\n",
    "Looking at the data from the 6 that didn't have incidents, no pattern is identified about their attributes.\n",
    "Different teams installed them, they have different materials, locations, weather and were operational on different years.\n",
    "The only similarity is that all these assets have a total useful life well below the 25% percentile.\n",
    "However, there are assets that had a shorter useful life and still had maintenance events.\n",
    "\n",
    "Let's check the statistics for time between events to decide whether to consider that these 6 assets are outliers and exclude them from further exploration.\n",
    "\n",
    "installed_date is the date of the last intervention, either installation, which matches the start_date in the assets table, or the last repair or replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec6367-cb69-494e-a1d6-e333620d8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "events['time_since_last_event'] = events['event_date'] - events['installed_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce67b5-b95c-45ba-bb5b-b9e10cc1a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "events['time_since_last_event'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b03aab-2d4d-4705-8e2d-9eaea7289dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0.9\n",
    "print(events['time_since_last_event'].quantile(q))\n",
    "print(f'# events over quantile {q}: {2032*(1-q)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41424a78-cd84-466e-9c8d-ee826068c2c0",
   "metadata": {},
   "source": [
    "For 10% of events (~203 events), the time elapsed since the previous event was higher than 321 days.\n",
    "Of the 6 assets than didn't have events, 5 have a useful life below 300 days.\n",
    "This means that it's plausible that these 6 assets didn't have any maintenance events, given their brief useful life, and we'll reject the hypothesis that it's due to missing data in the events table\n",
    "\n",
    "\n",
    "They will not be removed from the analysis when considering only assets' attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b37c74-1185-4a5d-b97c-b58545681331",
   "metadata": {},
   "source": [
    "## 1.4. Data Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1a23b-30fc-455f-8d86-5ac65fb56b03",
   "metadata": {},
   "source": [
    "The events and assets tables are two different kinds of data and we can use them to answer lots of questions.\n",
    "1. Which asset attributes are correlated with number of maintenance events?\n",
    "2. Which attributes are correlated with total useful life?\n",
    "3. Are there better performing teams?\n",
    "4. Should we be avoiding certain materials in specific locations or weather clusters?\n",
    "5. Can we predict the remaining useful life of an asset within several given time horizons (e.g. 30, 90, 180 days)?\n",
    "\n",
    "In section 2., we'll use the events data to augment the assets table and gather as much insights as possible about questions 1-4, and others that might arise during analysis.\n",
    "\n",
    "These insights will be used to guide question 5, in section 3.\n",
    "There, we'll build and describe a functional pipeline to predict approximate remaining useful life for each asset.\n",
    "**This pipeline can then be used in production, informing management decisions and guiding operation and maintenance teams in the field for reducing costs and downtime, and increase team productivity and customer satisfaction.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bff7d4-8c8d-4f56-a0e8-ba054b1d3896",
   "metadata": {},
   "source": [
    "# 2. Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ece22ed-d91b-466e-8dbf-0204259aad6f",
   "metadata": {},
   "source": [
    "## 2.1. Transforming features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d943b133-db88-4d2d-88b0-391c4b6084c6",
   "metadata": {},
   "source": [
    "### 2.1.1. Transforming categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8367b-32ab-40e0-91ad-c7a9f16db145",
   "metadata": {},
   "source": [
    "For computing the correlation of the different features, we have to transform some of them.\n",
    "Specifically, we'll need to transform categorical features (team, line, material).\n",
    "\n",
    "Let's make a copy of the original assets table and apply this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2094d-1ed6-4f6e-849d-10c3aed26bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets2 = assets.copy()\n",
    "events2 = events.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d431a32-5e0f-451a-bbea-bd5ae542241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets2 = pd.get_dummies(assets2, columns=['asset_material', 'asset_line', 'asset_weather_cluster', 'asset_install_team', 'asset_weather_cluster'])\n",
    "events2 = pd.get_dummies(events2, columns=['type', 'planned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537f259b-0ac0-4fad-8701-cc0074264c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beba1fb-f6e2-4227-99d1-f4d07729bd7c",
   "metadata": {},
   "source": [
    "### 2.1.2. Describing `previous_repairs` and `previous_unplanned`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0648311-8920-48a3-8843-c720dae4b2a0",
   "metadata": {},
   "source": [
    "The `previous_repairs` and `previous_unplanned` columns are present both in the events and assets tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914c5e4-19a2-4599-ba11-3f2bf613cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "events[events['asset_id'] == 'A:xoauw0'] .sort_values('event_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ede49e-525e-4b46-a143-d2a903ce62f8",
   "metadata": {},
   "source": [
    "`events`\n",
    "\n",
    "After studying the events sequence for several assets, the following was concluded.\n",
    "- In the events table, the previous_repairs column contains how many repair events ocurred since the last replacement event.\n",
    "- The previous_unplanned contains how many unplanned repair events occurred snce the last replacement event.\n",
    "- Each time a replacement happens, both counters are reset to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e6c93-28e3-4322-ac90-116244473535",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets[assets['start_previous_unplanned'] > 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06172c0-493f-4914-a76a-dda1f451e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_id = 'A:z7x72w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18de6f28-a099-4e80-bf35-38c3330298c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets[assets['asset_id'] == asset_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95859486-3b87-473a-a8b7-7871091ca42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "events[events['asset_id'] == asset_id] .sort_values('event_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda2c49-27a3-4540-a010-d57ea7160f06",
   "metadata": {},
   "source": [
    "`assets`\n",
    "After studying the events sequence for several assets and their corresponding assets row, the following was concluded:\n",
    "- The previous_repairs and previous_unplanned features of the assets table represen the total number repairs and number of unplanned repairs since the last replacement, at the moment of installation.\n",
    "- This can be observed, for example, in the data regarding the asset with ID \"A:z7x72w\".\n",
    "  - previous_repairs is 0 and previous_unplanned is 6 in the assets table. The first event is an unplanned repair. Both counters are incremented in the events table.\n",
    "- It should be noted that in the events table, previous_unplanned is always lower than previous_repairs, but in the assets table such is not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dcdf39-6af7-4f4f-9d12-063c733fcc5d",
   "metadata": {},
   "source": [
    "### 2.1.3 Transforming dates and time periods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2bd2e-ceb0-48b9-9f6c-f44092e3af19",
   "metadata": {},
   "source": [
    "We'll also need to transform features containing dates and time durations, because datetimes and timedeltas can't be directly correlated with other numeric data.\n",
    "\n",
    "- We'll make a feature from the year to try to understand if older installations are less reliable.\n",
    "- We'll use the month and day to make a sinusoidal wave with a period of one year and no phase shift, to understand if the seasons and time of the year also has an influence.*\n",
    "  - We use a sinusoid so that the last days from one year are similar to the first days from the following year.\n",
    "- We'll also add a weekday categorical variable (one hot encoded) to see understand if more \n",
    "\n",
    "\\* [method for convertion](https://math.stackexchange.com/a/650235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddcd56f-5d2c-49ed-bff8-4b41af25b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_of_year_to_sin(day: pd.Timestamp,\n",
    "                       period: float = 365.0,\n",
    "                       phase_shift: float = 84.0):\n",
    "    '''\n",
    "    day: a pandas timestamp\n",
    "    period: specifies the period of the wave\n",
    "    phase_shift: shifts the sine wave so the the peaks are at specific days, 84 makes the peak ~match the solstice\n",
    "    '''\n",
    "    return math.sin((2 * math.pi) / period * (day.day_of_year - phase_shift))\n",
    "    \n",
    "\n",
    "weekdays = {0: 'monday',\n",
    "            1: 'tuesday',\n",
    "            2: 'wednesday',\n",
    "            3: 'thursday',\n",
    "            4: 'friday',\n",
    "            5: 'saturday',\n",
    "            6: 'sunday'\n",
    "           }\n",
    "\n",
    "def augment_datetime(df: pd.DataFrame, columns: List[str]):\n",
    "    '''\n",
    "    for each pandas.Timestamp column in dataframe df, create 3 features:\n",
    "    - year (integer)\n",
    "    - sine wave (from day and month) with yearly period\n",
    "    - weekday, one hot encoded\n",
    "    for each pandas.Timedelta column, convert to integer in days\n",
    "    \n",
    "    :returns: the input with \n",
    "    '''\n",
    "    df_copy = df.copy()\n",
    "    for col in columns:\n",
    "        if col not in df_copy:\n",
    "            raise Exception(f'column {col} not present in dataframe')\n",
    "            \n",
    "        if isinstance(df_copy[col].iloc[0], pd.Timestamp):\n",
    "            df_copy[col + '_year'] = df_copy[col].map(lambda d: d.year)\n",
    "            df_copy[col + '_weekday'] = df_copy[col].map(lambda d: weekdays[d.day_of_week])\n",
    "            df_copy[col + '_sin_year'] = df_copy[col].map(day_of_year_to_sin)\n",
    "            \n",
    "            df_copy = pd.get_dummies(df_copy, columns=[col + '_weekday'])\n",
    "            \n",
    "        elif isinstance(df[col].iloc[0], pd.Timedelta):\n",
    "            df_copy[col + '_days_int'] = df_copy[col].map(lambda td: td.days)\n",
    "            \n",
    "        else:\n",
    "            raise TypeError(f'{col} is not of type pandas.Timestamp or pandas.Timedelta')\n",
    "            \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3eef2-3524-40c4-a023-0a05905d9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets2 = augment_datetime(assets2, ['end_date', 'start_date', 'total_useful_life'])\n",
    "events2 = augment_datetime(events2, ['event_date', 'installed_date', 'time_since_last_event'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef02f8-dbe8-4235-beac-1062cda90ff0",
   "metadata": {},
   "source": [
    "## 2.2. Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40709bf6-aad7-46a3-920a-4c81aaaa8d28",
   "metadata": {},
   "source": [
    "Let's augment the assets table with events statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06509f99-4c93-4c24-a231-5cf282b71ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns with default value 0, because of the 6 assets that don't have reported events\n",
    "assets2['total_repairs'] = 0\n",
    "assets2['total_replacements'] = 0\n",
    "\n",
    "assets2['total_unplanned_repairs'] = 0\n",
    "assets2['total_unplanned_replacements'] = 0\n",
    "\n",
    "assets2['average_time_between_events'] = 0\n",
    "assets2['std_time_between_events'] = 0\n",
    "\n",
    "assets2 = assets2.set_index('asset_id')\n",
    "\n",
    "events2['unplanned_repairs'] = events2['type_repair'] * events2['planned_False']\n",
    "events2['unplanned_replacements'] = events2['type_replacement'] * events2['planned_False']\n",
    "\n",
    "\n",
    "asset_group = events2.groupby('asset_id')\n",
    "\n",
    "assets2['total_repairs'] = asset_group['type_repair'].sum()\n",
    "assets2['total_replacements'] = asset_group['type_replacement'].sum()\n",
    "assets2['average_time_between_events'] = asset_group['time_since_last_event_days_int'].mean()\n",
    "assets2['std_time_between_events'] = asset_group['time_since_last_event_days_int'].std()\n",
    "\n",
    "\n",
    "assets2['total_unplanned_repairs'] = asset_group['unplanned_repairs'].sum()\n",
    "assets2['total_unplanned_replacements'] = asset_group['unplanned_replacements'].sum()\n",
    "\n",
    "events2.drop(columns=['unplanned_repairs', 'unplanned_replacements'])\n",
    "\n",
    "assets2[['total_repairs','total_replacements',\n",
    "         'average_time_between_events',\n",
    "         'std_time_between_events',\n",
    "         'total_unplanned_repairs',\n",
    "         'total_unplanned_replacements']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b5674-23b2-4b17-a6fc-1d11b5346fbb",
   "metadata": {},
   "source": [
    "## 2.3. Results and interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb96e77-d49e-454f-9f86-6a2afc5b1869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_filter(df: pd.DataFrame,\n",
    "                column: str,\n",
    "                threshold: float = 0.2) -> pd.Series:\n",
    "    corr = df.corr()[column]\n",
    "    return corr[(corr <= -threshold) | (corr >= threshold)].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee82a16-5e43-482e-8121-8137f3dd8593",
   "metadata": {},
   "source": [
    "### 2.3.1. How is the data distributed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f64d1a-4779-479c-b94f-bf5ad1c58d10",
   "metadata": {},
   "source": [
    "#### 2.3.1.1. Events types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f80ea6e-a881-477b-8fd2-a8c8e64211c2",
   "metadata": {},
   "source": [
    "The vast majority of events are unplanned $85\\%$, for both repairs and replacements ($33\\%$ and $52$, respectively).\n",
    "The number of replacements ($57\\%$) is slightly higher than that of repairs ($43\\%$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbe411-c125-45cc-a00f-9f94958ef33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.groupby(['type', 'planned']).count()['asset_id'] / events.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e94caa-d8c1-4d1a-a6b0-126219693784",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.groupby(['type', 'planned']).count()['asset_id'].plot(kind='bar')\n",
    "\n",
    "plt.title('Type of event')\n",
    "plt.ylabel('# events')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48ef5ed-e363-4859-9c3a-af8d6411bc85",
   "metadata": {},
   "source": [
    "#### 2.3.1.2. Asset line, weather, material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deff9a3-04d7-4b40-a04b-d275afa8611b",
   "metadata": {},
   "source": [
    "`weather clusters`\n",
    "- A \"standard\" weather cluster accounts for most of the assets ($46\\%$), but for the remaining clusters the assets are fairly well distributed.\n",
    "- Useful life for heavy rain is lower than for the other weather clusters, and highest for sun.\n",
    "However, the usage for heavy rain is highest and lowest for sun.\n",
    "This means we cannot conclude right away that it's a particular weather cluster that is having an influence on useful life.\n",
    "\n",
    "`asset lines`\n",
    "- Assets' distribution over asset lines is almost uniform.\n",
    "- Useful life is slightly higher for east and south lines.\n",
    "Again, these lines have a lower usage.\n",
    "\n",
    "`asset materials`\n",
    "- Assets' distribution over materials is close to uniform.\n",
    "- Steel has a slightly lower useful life, but is one of the materials with highest usage, so it makes sense.\n",
    "- Iron has a high useful life, even though it has a high usage, which is an interesting result.\n",
    "- Material variations for both usage and useful life are lower when compared to other variables.\n",
    "For this reason, high confidence conclusions should not be drawn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506d3ba-579f-4e3d-b5f9-e8647bd41ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets['total_useful_life'] = assets['total_useful_life'].map(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb27a9-b836-4bf7-9802-45c72a4e9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_useful_life = assets['total_useful_life'].mean()\n",
    "print(f'average useful life: {mean_useful_life}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45596ad6-c0b7-4129-ac38-f33353a1a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.groupby('asset_weather_cluster')['asset_id'].count().plot(kind='pie', autopct='%1.f%%')\n",
    "plt.title('Assets by weather cluster')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffaec5-9c3c-4820-b76c-1c9668c06caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.groupby('asset_weather_cluster')['asset_trains_per_hour'].mean().plot(kind='bar')\n",
    "plt.title('average usage per weather cluster')\n",
    "plt.ylabel('hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29006fc5-cbd0-46db-8cd6-f0144ae01110",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.groupby('asset_weather_cluster')['total_useful_life'].mean().plot(kind='bar')\n",
    "plt.title('average useful life per weather cluster')\n",
    "plt.ylabel('days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f5e53-b072-43e2-80af-2fa8c4c47148",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.groupby('asset_line')['asset_id'].count().plot(kind='pie', autopct='%1.f%%')\n",
    "plt.title('Assets by line')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d63be6-526d-499a-aabd-41cd06963038",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.groupby('asset_line')['asset_trains_per_hour'].mean().plot(kind='bar')\n",
    "plt.title('average usage per asset line')\n",
    "plt.ylabel('hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229cf82-fc70-4ba2-8cea-489b8085e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.groupby('asset_line')['total_useful_life'].mean().plot(kind='bar')\n",
    "plt.title('average useful life per weather cluster')\n",
    "plt.ylabel('days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50192664-69b1-4405-a6c5-159cef06dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.groupby('asset_material')['asset_id'].count().plot(kind='pie', autopct='%1.f%%')\n",
    "plt.title('Assets by line')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88287f-afbb-4442-b094-fd57f73bfd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.groupby('asset_material')['asset_trains_per_hour'].mean().plot(kind='bar')\n",
    "plt.title('average usage per asset material')\n",
    "plt.ylabel('hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b3642-6a3d-46bf-9698-8275984ebc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.groupby('asset_material')['total_useful_life'].mean().plot(kind='bar')\n",
    "plt.title('average useful life per asset material')\n",
    "plt.ylabel('days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adbef05-007b-4997-b2da-e807d6404bf0",
   "metadata": {},
   "source": [
    "#### 2.3.1.3. Asset usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399066e1-03c6-4ea8-b97d-ef8827d7028c",
   "metadata": {},
   "source": [
    "Since asset usage is distributed over a small set of values, we can plot this variable against others and try to understand some patterns. Asset usage was already studied above, so we'll analyze other relationships in these section.\n",
    "\n",
    "- Surprisingly, useful life doesn't decrease linearly with increasing asset usage (inverse relationship).\n",
    "This suggests that useful life has several different contributing factors.\n",
    "- The number of maintenance events is higher for higher usage, which would be expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa816e-16cf-4298-86e3-f92762ddc9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.groupby('asset_trains_per_hour')['total_useful_life'].mean().plot(kind='bar')\n",
    "plt.title('average useful life per asset usage')\n",
    "plt.ylabel('days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73642e2f-26f2-4dfc-94ae-165deb0bb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets2.groupby('asset_trains_per_hour')['total_repairs'].mean().plot(kind='bar')\n",
    "plt.title('repairs per asset usage')\n",
    "plt.ylabel('# events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b3f64-528f-41f2-8738-5a8609c7c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets2.groupby('asset_trains_per_hour')['total_replacements'].mean().plot(kind='bar')\n",
    "plt.title('replacements per asset usage')\n",
    "plt.ylabel('# events')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5986f-a258-47d5-905a-0d01c7c32e70",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3.2. What influences total useful life?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc6f27-b293-4169-92a1-4f64825efcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_filter(assets2, 'total_useful_life_days_int', threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d6e9f-353a-43c0-96d3-423a6dfd991e",
   "metadata": {},
   "source": [
    "- There is very week correlation between useful life and most other variables.\n",
    "- start_date has a strong correlation, but this likely means that the assets that were installed earlier have a better change to be represented longer in the events table.\n",
    "- The number of events (repair and replacements) is also strongly correlated.\n",
    "Again, an asset that is active for a long time is more likely to have more maintenance incidents, so this is to be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c93853-c45c-45bc-88c4-3d67720a3ae3",
   "metadata": {},
   "source": [
    "### 2.3.3. What influences occurence of maintenance events?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a77144-d8d2-4143-8a83-597d7589888b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a16b6e5-17d8-4e9f-8997-9af13851783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_filter(assets2, 'total_repairs', threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e88023-98b1-41cb-80e9-cd1966e8c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_filter(assets2, 'total_replacements', threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65fd2b-10aa-4a84-bd17-1b89ccc563d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_filter(assets2, 'total_unplanned_repairs', threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e21d5-d433-415f-a926-3cbd89a74102",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_filter(assets2, 'total_unplanned_replacements', threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bb00e-56d4-46d6-9a85-83de9879c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_filter(assets2, 'total_unplanned_repairs', threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc28d0a-5685-4f0c-bb78-d7a95da36aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "events2.corr()['type_repair'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ae348-b886-4714-ac05-4d9e3b703bc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3.4. Team performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b112c-7116-4cb6-8421-99800b0c49ce",
   "metadata": {},
   "source": [
    "**Assumption: the team that installed a particular asset is also the team that performed all repairs and replacements for that same asset throughout its useful life.**\n",
    "\n",
    "- Assets are well distributed over teams (close to uniform).\n",
    "- Events are well distributed over teams.\n",
    "Teams with more assets have more events.\n",
    "- The percentage of unplanned events is very close for all teams.\n",
    "- There is little variability within useful life, among teams.\n",
    "Of note, team 2 has a slightly lower average asset useful life, even though it has the lowest average asset usage, which would be more surprising, if values varied significantly.\n",
    "- The conclusion is that the effectiveness and relative competence of teams is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36026d-f1e5-4754-bab6-3180d301d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets2['team'] = assets.set_index('asset_id')['asset_install_team']\n",
    "assets2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf94ef-0f7b-432e-bc54-9b5d710ee91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets2.groupby('team')['total_useful_life'].count().plot(kind='pie', autopct='%1.f%%')\n",
    "plt.title('Asset distribution by team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84186dc6-ce58-4dff-bcfd-31d30039adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "events3 = pd.merge(events2, assets2, how='inner', on=['asset_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910f8b7-8dca-4b7f-ad0e-80e61d7e4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "events3.groupby('team')['event_id'].count().plot(kind='pie', autopct='%1.f%%')\n",
    "plt.title('Events distribution by team')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0306254c-f37e-4cef-9f22-437b22d87b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "(events3.groupby('team')['planned_False'].sum() / events3.groupby('team')['planned_False'].count() * 100).plot(kind='bar')\n",
    "plt.title('% of unplanned events by team')\n",
    "plt.ylabel('%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6699a-2841-4ae1-b9c2-d088f7a5de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.groupby('asset_install_team')['total_useful_life'].mean().plot(kind='bar')\n",
    "plt.title('average useful life per team')\n",
    "plt.ylabel('days')\n",
    "plt.xlabel('team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5029ea3-4e67-4942-abe3-edaf86035157",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.groupby('asset_install_team')['asset_trains_per_hour'].mean().plot(kind='bar')\n",
    "plt.title('asset usage per team')\n",
    "plt.ylabel('hours')\n",
    "plt.xlabel('team')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b04dd60-d2e7-4f43-a3ee-d2284b6dcd88",
   "metadata": {},
   "source": [
    "### 2.3.5. Asset usage and event occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ed437-0dcd-4e0f-a720-fe13e57879f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(events3['event_date'].iloc[0] - events3['installed_date'].iloc[0]).days * 24 * events3['asset_trains_per_hour'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbac75c-4c4a-465b-a6dc-71d7d01c5371",
   "metadata": {},
   "outputs": [],
   "source": [
    "events3['trains_since_installed'] = (events3['event_date'] - events3['installed_date']).map(lambda x: x.days) * 24 * events3['asset_trains_per_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf0548-1cdd-4f01-807c-55ee2ab3a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "events3.corr()['type_repair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec70663-b411-451e-a552-6a38ac03d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "events3['trains_since_installed'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9fc93-4326-4f7a-9584-d2277cf21a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "events3[events3['type_replacement'] == 1]['trains_since_installed'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8878a7c-8bd5-493e-ac2c-10fcf4df8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "events3[events3['type_replacement'] == 0]['trains_since_installed'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485cb50-1736-426d-9656-4910512e03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "events3[events3['planned_False'] == 1]['trains_since_installed'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ba087-190b-4d55-8747-df69492e452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "events3[events3['planned_False'] == 0]['trains_since_installed'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3cbf09-d94c-4d8b-ada0-f82541372a09",
   "metadata": {},
   "source": [
    "# 3. Predictive maintenance pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eacdaa-ce88-4f52-9d99-4068c214b02a",
   "metadata": {},
   "source": [
    "The main goal with the predictive model is to reduce the number of unplanned events, since those drastically increase cost, downtime and liability.\n",
    "Since most ot the events are unplanned ($85\\%$), we have a **class imbalance**.\n",
    "We'll make an assumption that will allow the simplification of the model.\n",
    "\n",
    "**Assumptions**\n",
    "- Models must predict occurence of events, the technical team will inspect the asset and determine one of three alternatives:\n",
    "  - false positive (in time, the feedback data can be used to modify and further train the model)\n",
    "  - repair necessary\n",
    "  - replacement necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61072096-fed8-4e8e-9975-7162adc72848",
   "metadata": {},
   "source": [
    "**Strategy**\n",
    "- We can have different models, one for each time horizon and they'll be binary class models.\n",
    "- Or, we can have a multi class model, where each time horizon is a different class.\n",
    "\n",
    "**Plan**\n",
    "- Augment events dataset\n",
    "  - with data from assets table.\n",
    "  - to indicate number of planned repair events in last 30/90/180/360 days\n",
    "  - to indicate number of unplanned repair events in last 30/90/180/360 days\n",
    "  - to indicate number of planned replacement events in last 30/90/180/360 days\n",
    "  - to indicate number of unplanned replacement events in last 30/90/180/360 days\n",
    "  - multiply usage per hour by the total number of hours\n",
    "  - current life (event_date - start_date)\n",
    "  - difference between average useful life and current life (average useful life - current_life)\n",
    "    - average useful life must be updated when more data is collected\n",
    "    - upgrade to use average useful life for events that match the features of the event being trained/inferred (e.g. weather, usage, team, material etc.)\n",
    "- Add columns for indicating whether another event will happen in different time horizons: 30, 90, 180, 360 days - this is the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb0ac6-cb77-4c71-96c9-9e23c649fc56",
   "metadata": {},
   "source": [
    "## 3.1. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012306b-8de3-4662-9e97-44bf7241c5f1",
   "metadata": {},
   "source": [
    "As it is, the dataset is not ideal for creating a model.\n",
    "There are not many variables with high correlation.\n",
    "Moreover, we only have data for when the event already occurred, that is \n",
    "\n",
    "The dataset can be extended to include entries with dates between consequent events.\n",
    "For example, consider that an event occurred in the 1st of march and then again in the 1st of may.\n",
    "We can create 30 data points covering the month of january, with the indication that no event will happen in the next 30 days, but an event will happen in the next 90 days.\n",
    "We can also create 30 data points for the month of february, with the indication that an event will happen in the next 30 days.\n",
    "\n",
    "With this strategy, we'll be able to build a dataset that will contain a lot more rows than the events table.\n",
    "To be specific, since the average useful life is 1530 days and we have 200 assets, we'll be able to build a dataset in the order of 300 000 rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb0872-e5c2-4b1e-b6cf-d12ebdcb8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "events3['time_since_last_event'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e8af12-ad46-4703-ac28-4dcb57bee6af",
   "metadata": {},
   "source": [
    "Features:\n",
    " - days since start_date - this will be a proxy for date (must be derived)\n",
    " - days since last installed date (must be derived)\n",
    " - trains since last installed date (must be derived)\n",
    " - previous repairs since last replacement (original data)\n",
    " - previous unplanned repairs since last replacement (original data)\n",
    " - number of previous replacements (must be derived)\n",
    "\n",
    "Target\n",
    " - event in next 30 days (must be derived)\n",
    " - event in next 90 days (must be derived)\n",
    " - event in next 180 days (must be derived)\n",
    " - event in next 360 days (must be derived) (this might not make sense, since 75% of events happen within less than 181 days of each other)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a897e2-c34d-4d3a-ac07-2bf736992e51",
   "metadata": {},
   "source": [
    "### 3.1.1. Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85115950-7db1-4e59-a1eb-a4e09bf1b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_in_X_time(date: pd.Timestamp,\n",
    "                    asset_data: pd.DataFrame,\n",
    "                    horizon: pd.Timedelta = pd.Timedelta(days=30)\n",
    "                   ) -> bool:\n",
    "    return (asset_data['event_date'] < (date + horizon)).sum() > 0\n",
    "\n",
    "def get_last_event(date: pd.Timestamp,\n",
    "                   asset_data: pd.DataFrame) -> pd.Series:\n",
    "    prev_events = asset_data[asset_data['event_date'] < date]\n",
    "    if prev_events.shape[0] == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return prev_events.iloc[-1]\n",
    "    \n",
    "    \n",
    "EventCounts = namedtuple('EventCounts', ['planned_repairs', 'unplanned_repairs', 'planned_replacements', 'unplanned_replacements'])\n",
    "def get_past_events_count(date: pd.Timestamp,\n",
    "                          asset_data: pd.DataFrame) -> EventCounts:\n",
    "    prev_events = asset_data[asset_data['event_date'] < date]\n",
    "    planned_repairs = (prev_events['type_repair'] * prev_events['planned_True']).sum()\n",
    "    unplanned_repairs = (prev_events['type_repair'] * prev_events['planned_False']).sum()\n",
    "    planned_replacements = (prev_events['type_replacement'] * prev_events['planned_True']).sum()\n",
    "    unplanned_replacements = (prev_events['type_replacement'] * prev_events['planned_False']).sum()\n",
    "\n",
    "    return EventCounts(planned_repairs=planned_repairs,\n",
    "                       unplanned_repairs=unplanned_repairs,\n",
    "                       planned_replacements=planned_replacements,\n",
    "                       unplanned_replacements=unplanned_replacements\n",
    "                      )\n",
    "    \n",
    "def build_dataset(asset_data: pd.DataFrame,\n",
    "                  horizons: List[int] = [30, 60, 180],\n",
    "                  static_columns: List[str] = [],\n",
    "                  start_date: Optional[pd.Timestamp] = None,\n",
    "                  end_date: Optional[pd.Timestamp] = None) -> pd.DataFrame:\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # static_data\n",
    "    asset_start_date = asset_data.iloc[0]['start_date']\n",
    "    trains_per_hour = asset_data.iloc[0]['asset_trains_per_hour']\n",
    "    \n",
    "    # create date range\n",
    "    first_day = asset_start_date if start_date is None else start_date\n",
    "    last_day = asset_data.iloc[-1]['event_date'] if end_date is None else end_date\n",
    "    df['date'] = pd.date_range(start=first_day,\n",
    "                               end=last_day)\n",
    "    \n",
    "    df['asset_id'] = asset_data.iloc[0]['asset_id']\n",
    "    \n",
    "    ## loop dates instead of using map for performance\n",
    "    ## (not enough data to justify efficient parallelization)\n",
    "    \n",
    "    # create columns\n",
    "    df['days_since_asset_start'] = 0\n",
    "    df['days_since_last_event'] = 0\n",
    "    \n",
    "    df['repairs_since_replacement'] = 0\n",
    "    df['unplanned_repairs_since_replacement'] = 0\n",
    "    \n",
    "    df['unplanned_repairs'] = 0\n",
    "    df['planned_repairs'] = 0\n",
    "    df['unplanned_replacements'] = 0\n",
    "    df['planned_replacements'] = 0\n",
    "    \n",
    "    \n",
    "    for horizon_days in horizons:\n",
    "        df[f'horizon_{horizon_days}'] = 0\n",
    "        \n",
    "    for i, d in df['date'].items():\n",
    "        # print(d)\n",
    "        last_event = get_last_event(d, asset_data)\n",
    "        \n",
    "        # days since asset start date\n",
    "        df.at[i, 'days_since_asset_start'] = (d - asset_start_date).days\n",
    "\n",
    "        # days since last event\n",
    "        df.at[i, 'days_since_last_event'] = (d - last_event['event_date']).days if last_event is not None else 0\n",
    "     \n",
    "        # previous repairs since last replacement\n",
    "        df.at[i, 'repairs_since_replacement'] = last_event['previous_repairs'] if last_event is not None else 0\n",
    "    \n",
    "        # previous unplanned repairs since last replacement\n",
    "        df.at[i, 'unplanned_repairs_since_replacement'] = last_event['previous_unplanned'] if last_event is not None else 0\n",
    "        \n",
    "        \n",
    "        event_counts = get_past_events_count(d, asset_data)\n",
    "        # total previous unplanned repairs\n",
    "        df.at[i, 'unplanned_repairs'] = event_counts.unplanned_repairs\n",
    "        \n",
    "        # total previous planned repairs\n",
    "        df.at[i, 'planned_repairs'] = event_counts.planned_repairs\n",
    "        \n",
    "        # total previous unplanned replacements\n",
    "        df.at[i, 'unplanned_replacements'] = event_counts.unplanned_replacements\n",
    "        \n",
    "        # total previous planned replacements\n",
    "        df.at[i, 'planned_replacements'] = event_counts.planned_replacements\n",
    "        \n",
    "        # check existence of events in specified horizons (days)\n",
    "        for horizon_days in horizons:\n",
    "            df.at[i, f'horizon_{horizon_days}'] = event_in_X_time(d, asset_data,\n",
    "                                                                    pd.Timedelta(days=horizon_days))\n",
    "     \n",
    "    # trains since last event\n",
    "    df['trains_since_last_event'] = df['days_since_last_event'] * 24 * trains_per_hour\n",
    "\n",
    "    # trains since asset start date\n",
    "    df['trains_since_asset_start'] = df['days_since_asset_start'] * 24 * trains_per_hour\n",
    "\n",
    "    # add data from static columns\n",
    "    for col in static_columns:\n",
    "        if col not in asset_data:\n",
    "            raise ValueError(f'column {col} does not exist in asset data')\n",
    "        df[col] = asset_data.iloc[0][col]\n",
    "        \n",
    "    return df\n",
    "\n",
    "def get_asset_data(asset_id, events_data) -> pd.DataFrame:\n",
    "    return events_data[events_data['asset_id'] == asset_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86349c8c-ff08-41ee-a861-ea18388c8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset cration parameteres\n",
    "params = {\n",
    "    'horizons': [30, 60, 90, 180, 360],\n",
    "    'static_columns': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8969919-fcf7-48ea-87c7-fbab71b71f6c",
   "metadata": {},
   "source": [
    "#### 3.1.1.1. Create and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84e810-e07d-4bff-9cf4-c1d16f72a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "unique_ids = events['asset_id'].unique()\n",
    "\n",
    "# first asset\n",
    "asset_data = get_asset_data(unique_ids[0], events3)\n",
    "df = build_dataset(asset_data, **params)\n",
    "\n",
    "# remaing assets\n",
    "for i, asset_id in enumerate(unique_ids[1:2]):\n",
    "    print(i, asset_id, datetime.datetime.now())\n",
    "    asset_data = get_asset_data(asset_id, events3)\n",
    "    d = build_dataset(asset_data, **params)\n",
    "    df = pd.concat([df, d], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f15526-7a99-42f4-a321-175c4a797cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = assets[['asset_id', 'asset_install_team', 'asset_line', 'asset_material', 'asset_weather_cluster', 'end_date',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8914b4-efea-45ab-bf9f-02fccdf02453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, static_df, on='asset_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc86cae-b500-41dd-8426-c906ce7ddb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remaining useful life\n",
    "df['rul'] = (df['end_date'] - df['date']).map(lambda d: d.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f17e2a-ba3c-4722-b48f-7b6a9b83c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f'created_data_{datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a1a8f3-0b88-4a65-8ec7-5b2272bc46c4",
   "metadata": {},
   "source": [
    "#### 3.1.1.2. Load previous creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe4df4-96dd-4a11-a369-e39f797ae76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('created_data_2022_05_02_22_50.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa023d85-6127-4909-9158-a975e40955cf",
   "metadata": {},
   "source": [
    "### 3.1.2. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4825080-862d-4617-a893-ad52fb729cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for horizon in params['horizons']:\n",
    "    df[f'horizon_{horizon}'] = df[f'horizon_{horizon}'].map(lambda x: 1 if x else 0)\n",
    "    \n",
    "    # measure class imbalance\n",
    "    pct = df[f'horizon_{horizon}'].sum() / df.shape[0]\n",
    "    print(f'class horizon {horizon}: {pct*100}% positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57816e-7bbf-4d8c-827e-4d86486a2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['horizon_30'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ec87e-5451-4327-a734-699f16615244",
   "metadata": {},
   "source": [
    "We have a high class imbalance.\n",
    "Let's down sample the dataset to have a balanced class distribution.\n",
    "We only have about 25000 samples that don't have an event in the future 30 days.\n",
    "Let's downsample the one that have to that amount too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c05f3f-df46-493f-8880-a5980aa17917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_balanced_sample(x: pd.DataFrame, target: str, tolerance: float = 0.1) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    :param x: input dataset\n",
    "    :type x: pandas.DataFrame\n",
    "    :param target: column name of binary class\n",
    "    :type target: str\n",
    "    :returns: balanced dataset, \n",
    "    '''\n",
    "    unique_values = x[target].unique()\n",
    "    if unique_values.size != 2:\n",
    "        raise TypeError(f'{target} has {unique_values.size} classes, not 2')\n",
    "    \n",
    "    idx = [x[x[target] == c].index for c in unique_values]\n",
    "    small_class_idx, big_class_idx = sorted(idx, key=lambda k: k.size)\n",
    "    \n",
    "    # check if already balanced\n",
    "    \n",
    "    # balance by sampling from the big class the number of rows in the small class\n",
    "    big_sampled = x.iloc[big_class_idx].sample(small_class_idx.size)\n",
    "    x_balanced = pd.concat([big_sampled, x.iloc[small_class_idx]])\n",
    "    left_out = x.iloc[x.index.difference(x_balanced.index)]\n",
    "    \n",
    "    return x_balanced, left_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d51fb8e-f05d-43ae-b8cf-8d38266c7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced, left_out = binary_balanced_sample(df, 'horizon_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256d2bb-f175-4587-9f6f-7b849dfb4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4fa4c9-4626-49f4-bd2b-caa82d0958ee",
   "metadata": {},
   "source": [
    "1. **dimensionaly reduction?**\n",
    "2. **which features?**\n",
    "3. **k-folds?**\n",
    "4. **which model?**\n",
    "5. **grid/random search?**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811dc107-f520-44ee-9e95-0943f281c9a2",
   "metadata": {},
   "source": [
    "### 3.1.3. Data split and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b32a83-2f6d-461c-adbb-7ae6fcfacb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = df\n",
    "print(dfX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c030bab-18c8-4ad1-b501-82a40b865835",
   "metadata": {},
   "source": [
    "Since we have a decently sized dataset, a simple train/validation split should suffice to get decent results, i.e. k-fold is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfdca78-5041-4fbe-bb15-914509e7714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb8352-7382-4152-bfe3-45d74b248108",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_target = ['horizon_30', 'horizon_60', 'horizon_90', 'horizon_180', 'horizon_360']\n",
    "\n",
    "columns_infer = [\n",
    "    'days_since_asset_start',\n",
    "    'days_since_last_event',\n",
    "    'previous_repairs',\n",
    "    'previous_unplanned',\n",
    "    'trains_since_last_event',\n",
    "    'trains_since_asset_start'\n",
    "]\n",
    "\n",
    "\n",
    "def getXy(data: pd.DataFrame, train_cols: List[str], target: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    return data[train_cols], data[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a1b83-f1ef-423e-adc6-b4105ef5d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = columns_target[0]\n",
    "\n",
    "# balance chosen dataset according to specified target column\n",
    "df_balanced, left_out = binary_balanced_sample(dfX, target_col)\n",
    "\n",
    "# split into \"inference\" set and target set\n",
    "X, y = getXy(df_balanced, columns_infer, target_col)\n",
    "\n",
    "# split left out set into validation inference/target sets\n",
    "Xval, yval = getXy(left_out, columns_infer, target_col)\n",
    "\n",
    "# train/test split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(f\"{df_balanced.shape[0]} \\t rows in balanced datset\")\n",
    "print(f\"{Xtrain.shape[0]} \\t rows in train set\")\n",
    "print(f\"{Xtest.shape[0]} \\t rows in test set\")\n",
    "print(f\"{Xval.shape[0]} \\t rows in validation set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405fe656-3a09-4946-acf0-af7da0ccb712",
   "metadata": {},
   "source": [
    "## 3.2. Single Class Multi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4139a-daa3-4c58-a998-f11680d316f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(ytrue: np.array, ypred: np.array) -> dict:\n",
    "    acc = metrics.accuracy_score(ytrue, ypred)\n",
    "    recall = metrics.recall_score(ytrue, ypred)\n",
    "    confusion = confusion_matrix(ytrue, ypred)\n",
    "    return {'accuracy': acc, 'recall': recall, 'confusion': confusion}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e7824-733d-421e-b919-4d5fc2916a3b",
   "metadata": {},
   "source": [
    "### 3.2.1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bfab41-4d8e-4ce6-b3f3-7009bb7ba4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta = {\n",
    "    'type': 'DecisionTreeClassifier',\n",
    "    'params': {\n",
    "        'max_depth': 1,\n",
    "        'criterion': 'entropy'\n",
    "    }\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{model_meta['type']}_{target_col}\"):\n",
    "    dt = DecisionTreeClassifier(**model_meta['params'])\n",
    "    dt.fit(Xtrain, ytrain)\n",
    "\n",
    "    ytrain_pred = dt.predict(Xtrain)\n",
    "    ytest_pred = dt.predict(Xtest)\n",
    "    yval_pred = dt.predict(Xval)\n",
    "    \n",
    "    run_metrics = {}\n",
    "    run_metrics['train'] = compute_metrics(ytrain, ytrain_pred)\n",
    "    run_metrics['test'] = compute_metrics(ytest, ytest_pred)\n",
    "    run_metrics['val'] = compute_metrics(yval, yval_pred)\n",
    "    \n",
    "    ConfusionMatrixDisplay(run_metrics['test']['confusion']).plot()\n",
    "    plt.title('Confusion matrix for test set')\n",
    "    figCT = plt.gcf()\n",
    "\n",
    "    ConfusionMatrixDisplay(run_metrics['val']['confusion']).plot()\n",
    "    plt.title('Confusion matrix for validation set')\n",
    "    figCV = plt.gcf()\n",
    "    \n",
    "    figFI = plt.figure()\n",
    "    plt.barh(list(X.columns), dt.feature_importances_)\n",
    "    plt.title('Feature importance')\n",
    "    plt.xlabel('Gini importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    figDP = plt.figure()\n",
    "    plt.title('Decision path')\n",
    "    tree.plot_tree(dt)\n",
    "    \n",
    "    # log figures\n",
    "    mlflow.log_figure(figCT, \"test_confusion.png\")\n",
    "    mlflow.log_figure(figCV, \"val_confusion.png\")\n",
    "    mlflow.log_figure(figFI, \"feature_importance.png\")\n",
    "    mlflow.log_figure(fig, \"decision_path.png\")\n",
    "\n",
    "    # log datasets size\n",
    "    mlflow.log_param('n_features', Xtrain.shape[1])\n",
    "    mlflow.log_param('train_size', Xtrain.shape[0])\n",
    "    mlflow.log_param('test_size', Xtest.shape[0])\n",
    "    mlflow.log_param('val_size', Xval.shape[0])\n",
    "    \n",
    "    # log params\n",
    "    for param, val in model_meta['params'].items():   \n",
    "        mlflow.log_param(param, val)\n",
    "    \n",
    "    # log metrics\n",
    "    for s in ['train', 'test', 'val']:\n",
    "        for m in ['accuracy', 'recall']:\n",
    "            print(f\"{s}_{m} - {run_metrics[s][m]}\")\n",
    "            mlflow.log_metric(f\"{s}_{m}\", run_metrics[s][m])\n",
    "\n",
    "            \n",
    "    tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "    \n",
    "    # Model registry does not work with file store\n",
    "    if tracking_url_type_store != \"file\":\n",
    "        mlflow.sklearn.log_model(dt, \"model\", registered_model_name=\"DecisionTree_\" + target_col)\n",
    "    else:\n",
    "        mlflow.sklearn.log_model(dt, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1460f-d42d-4cd8-bf7b-3e6be3cc0303",
   "metadata": {},
   "source": [
    "### 3.2.2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396bbbe3-e1d9-4f98-ae02-6e89ebfcbe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta = {\n",
    "    'type': 'RandomForestClassifier',\n",
    "    'params': {\n",
    "        'max_depth': None,\n",
    "        'n_estimators': 10,\n",
    "        'criterion': 'gini'\n",
    "    }\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{model_meta['type']}_{target_col}\"):\n",
    "    dt = RandomForestClassifier(**model_meta['params'])\n",
    "    dt.fit(Xtrain, ytrain)\n",
    "\n",
    "    ytrain_pred = dt.predict(Xtrain)\n",
    "    ytest_pred = dt.predict(Xtest)\n",
    "    yval_pred = dt.predict(Xval)\n",
    "    \n",
    "    run_metrics = {}\n",
    "    run_metrics['train'] = compute_metrics(ytrain, ytrain_pred)\n",
    "    run_metrics['test'] = compute_metrics(ytest, ytest_pred)\n",
    "    run_metrics['val'] = compute_metrics(yval, yval_pred)\n",
    "    \n",
    "    ConfusionMatrixDisplay(run_metrics['test']['confusion']).plot()\n",
    "    plt.title('Confusion matrix for test set')\n",
    "    figCT = plt.gcf()\n",
    "\n",
    "    ConfusionMatrixDisplay(run_metrics['val']['confusion']).plot()\n",
    "    plt.title('Confusion matrix for validation set')\n",
    "    figCV = plt.gcf()\n",
    "    \n",
    "    figFI = plt.figure()\n",
    "    plt.barh(list(X.columns), dt.feature_importances_)\n",
    "    plt.title('Feature importance')\n",
    "    plt.xlabel('Gini importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "    # log figures\n",
    "    mlflow.log_figure(figCT, \"test_confusion.png\")\n",
    "    mlflow.log_figure(figCV, \"val_confusion.png\")\n",
    "    mlflow.log_figure(figFI, \"feature_importance.png\")\n",
    "   \n",
    "    \n",
    "    # log params\n",
    "    for param, val in model_meta['params'].items():   \n",
    "        mlflow.log_param(param, val)\n",
    "        \n",
    "    for s in ['train', 'test', 'val']:\n",
    "        for m in ['accuracy', 'recall']:\n",
    "            print(f\"{s}_{m} - {run_metrics[s][m]}\")\n",
    "            mlflow.log_metric(f\"{s}_{m}\", run_metrics[s][m])\n",
    "\n",
    "    tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "    # Model registry does not work with file store\n",
    "    if tracking_url_type_store != \"file\":\n",
    "\n",
    "        # Register the model\n",
    "        # There are other ways to use the Model Registry, which depends on the use case,\n",
    "        # please refer to the doc for more information:\n",
    "        # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "        mlflow.sklearn.log_model(dt, \"model\", registered_model_name=\"DecisionTree_\" + target_col)\n",
    "    else:\n",
    "        mlflow.sklearn.log_model(dt, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6300900e-2607-42db-b39e-5befe07d9034",
   "metadata": {},
   "source": [
    "## 3.3. Multi Class Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b7602-ae41-4c95-9b04-1d52ebb148e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fe04829-2fd8-44b9-b089-e2db842e85ae",
   "metadata": {},
   "source": [
    "## 3.4. Predict Remaining Useful Life (RUL) (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3e8d6-6a69-47c2-91c5-98ba72bf8e59",
   "metadata": {},
   "source": [
    "The Remaining Useful Life is the amount of time left until the retirement of an asset.\n",
    "\n",
    "**Assumption: the retirement of an asset, here, is the end_date in the assets table.**\n",
    "\n",
    "At any given day, the retirement of an asset is that day subtracted from the end date.\n",
    "\n",
    "In the first models, we tried to predict whether a maintenance event would occur withing a given time frame.\n",
    "Here, we'll try to predict the RUL of a given asset, given what we know about it know.\n",
    "Some of this knowledge is static, e.g. material, line, weather cluster, team.\n",
    "Other is derived, e.g. how many trains passed since the beginning, how many repairs, how many replacements.\n",
    "These derived features have already been computed during the dataset creation, in section 3.1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06decdf7-385c-4641-adc4-a5482e638799",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b602eb-06ba-410f-a67e-e64efdbc0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408bcca-5ed5-441b-b0ca-0985f7b0a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = dfX[['trains_since_asset_start', 'rul']].sort_values('trains_since_asset_start')\n",
    "x = xx['trains_since_asset_start'].to_numpy()\n",
    "y = xx['rul'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601b1a0-2be4-4e7f-b064-54bcd7d55d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[::10],y[::10])\n",
    "plt.ylabel('RUL')\n",
    "plt.xlabel('trains_since_asset_start')\n",
    "plt.title('RUL/Trains since asset start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0edbed-a1b3-409d-8450-ad09748f9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# let us train the model\n",
    "\n",
    "m.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f5c2ed-031f-429f-9113-9ba34a11d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = m.predict(x)\n",
    "metrics.mean_absolute_error(y, ŷ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
